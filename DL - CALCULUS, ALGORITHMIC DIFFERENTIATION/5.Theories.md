## Differential Calculus

### Basic principle
>Calculus is a discipline of mathematics that provides us with tools to analyze rates of change, or decay, or motion. In simple terms, differential calculus is focused on instantaneous rates of change or computing the slope of a linear function.

>**the derivative a function is a function’s instantaneous rate of change**

"rate of change" as the **slope** of a function:
![[Pasted image 20220211020429.png]]
i.e.
![[Pasted image 20220211020453.png]]
and 
![[Pasted image 20220211020543.png]]
finally:
![[Pasted image 20220211020508.png]]

Note:
![[Pasted image 20220216204704.png]]

### Rules
![[Pasted image 20220211020643.png]]
![[Pasted image 20220211020655.png]]

### Chain rule
![[Pasted image 20220211021033.png]]

### Differentiablity
![[Pasted image 20220211023432.png]]
#### ReLU
![[Pasted image 20220211023458.png]]

### Partial Derivatives
![[Pasted image 20220211023834.png]]
and for second derivative:
![[Pasted image 20220211024435.png]] e.g. ![[Pasted image 20220211024600.png]]

### Multivariable chain rule
![[Pasted image 20220211025542.png]]

### Hessian 
![[Pasted image 20220211025740.png]]

### Laplacian
![[Pasted image 20220211030138.png]]
In general, for a multivariable function with $n$ arguments:
![[Pasted image 20220211030250.png]]


### References
1. [appendix_d_calculus.pdf (sebastianraschka.com)](https://sebastianraschka.com/pdf/books/dlb/appendix_d_calculus.pdf) or [[appendix_d_calculus.pdf]]



## Backpropagation

we’re going to use a neural network with two inputs, two hidden neurons, two output neurons. Additionally, the hidden and output neurons will include a bias:
![[Pasted image 20220215200944.png]]

### The Forward Pass
$$
\begin{align*} 
net_{h} &= IW_h \\
out_{h} &= \text{sigmoid}(net_h)\\
&\\
net_o &= out_h W_o\\
out_o &= \text{sigmoid}(net_o)\\
\end{align*}
$$

### Total Error = Loss function
$$
\begin{align*} 
E_{total} &= \sum{ \frac{1}{2} (\text{target} - out_o)}
\end{align*}
$$
>The $\frac{1}{2}$ is included so that exponent is cancelled when we differentiate later on. The result is eventually multiplied by a learning rate anyway so it doesn’t matter that we introduce a constant here.

### The Backwards Pass
Consider $w_5$. We want to know how much a change in $w_5$ affects the total error, aka $\frac{\partial E_{total}}{\partial W_5}$


$$
\begin{align*} 
\frac{\partial E_{total}}{\partial W_o} &= \frac{\partial E_{total}}{\partial out_o}. \frac{\partial out_o}{\partial net_o}.\frac{\partial net_o}{\partial W_o}
\end{align*}
$$

![[Pasted image 20220215202811.png]]


$$
\begin{align*} 
\frac{\partial E_{total}}{\partial out_o} &= 2 . \frac{1}{2}(\text{target} - out_o). (0 - 1)\\
\frac{\partial E_{total}}{\partial out_o} &= - (\text{target} - out)\\ 
\end{align*}
$$

$$
\begin{align*} 
\frac{\partial out_o}{\partial net_o} &= \text{sigmoid}(net_o). (1-\text{sigmoid}(net_o))\\
\frac{\partial out_o}{\partial net_o} &= out_o . (1-out_o)\\
\end{align*}
$$

$$
\begin{align*} 
\frac{\partial net_o}{\partial W_o} &= out_h\\
\end{align*}
$$


#### Delta Rule
>You’ll often see this calculation combined in the form of the [delta rule](http://en.wikipedia.org/wiki/Delta_rule):
$$
\begin{align*} 
\frac{\partial E_{total}}{\partial W_o} &= \frac{\partial E_{total}}{\partial out_o}. \frac{\partial out_o}{\partial net_o}.\frac{\partial net_o}{\partial W_o}\\
\frac{\partial E_{total}}{\partial W_o} &= - (\text{target} - out).( out_o(1-out_o)).(out_h)\\
\end{align*}
$$

#### Node delta ($\delta$ )
^024e2f
$$
\begin{align*}
\frac{\partial E_{total}}{\partial out_o}. \frac{\partial out_o}{\partial net_o} &= \frac{\partial E_{total}}{\partial net_o} = \delta_o\\
\frac{\partial E_{total}}{\partial net_o} = \delta_o.out_h
\end{align*}
$$

^a7827f

To update weights:
$$
\begin{align*}
W^+_o &= W_o - \eta\frac{\partial E_{total}}{\partial W_o}
\end{align*}
$$

^407271

#### For hidden layer
$$
\begin{align*} 
\frac{\partial E_{total}}{\partial W_h} &= \frac{\partial E_{total}}{\partial out_h}. \frac{\partial out_h}{\partial net_h}.\frac{\partial net_h}{\partial W_h}\\
\\ &where\\ \\
\frac{\partial E_{total}}{\partial out_h} &= \sum \frac{\partial E_{i}}{\partial out_h}\\
\\ &and\\ \\
\frac{\partial E_{i}}{\partial out_h} &= \frac{\partial E_{i}}{\partial net_o}. \frac{\partial net_o}{\partial out_h}
\end{align*}
$$
![[Pasted image 20220215205115.png]]

And from [[5.Theories#^a7827f]] we know that:
$$
\begin{align*}
\frac{\partial E_{i}}{\partial net_o} &= \frac{\partial E_{i}}{\partial out_o}. \frac{\partial out_o}{\partial net_o}\\
\\ &\text{and since}\\ \\
\frac{\partial net_o}{\partial out_h} &= W_o
\end{align*}
$$
We can compute $\frac{\partial E_{total}}{\partial out_h}$.

Now, we need to compute $\frac{\partial out_h}{\partial net_h}$:
$$
\begin{align*} 
\frac{\partial out_h}{\partial net_h} &= \text{sigmoid}(net_h). (1-\text{sigmoid}(net_h))\\
\frac{\partial out_h}{\partial net_h} &= out_o (1-out_h)\\
\end{align*}
$$
and 
$$
\begin{align*} 
\frac{\partial net_h}{\partial W_h} &= out_{in}\\
\end{align*}
$$

Now we can compute the entire backward pass for hidden layer. Also, this can be written in delta form using following equations:
$$
\begin{align*} 
\frac{\partial E_{total}}{\partial W_h} &= (\sum_o \frac{\partial E_{i}}{\partial out_o}. \frac{\partial out_o}{\partial net_o}. \frac{\partial net_o}{\partial out_h} ). \frac{\partial out_h}{\partial net_h}.\frac{\partial net_h}{\partial W_h}\\

\frac{\partial E_{total}}{\partial W_h} &= (\sum_o \delta_oW_o).(out_h(1-out_h)).out_{in}\\

\frac{\partial E_{total}}{\partial W_h} &= \delta_h.out_{in}

\end{align*}
$$
See [A Step by Step Backpropagation Example – Matt Mazur](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) if you need a numerical example.
### Reference
1. [A Step by Step Backpropagation Example – Matt Mazur](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) all images and equations have been borrowed from here.

## Taylor Series
The idea behind a Taylor series is that if you know a function and all its derivatives at one point $x = a$, you can approximate the function at other points near $a$.
 
![[Pasted image 20220217225407.png]]
![[Pasted image 20220217225417.png]]
![[Pasted image 20220217225432.png]] -> ![[Pasted image 20220217225441.png]]
![[Pasted image 20220217225511.png]]


### References
1. [An Easy Way to Remember the Taylor Series Expansion | by Andrew Chamberlain, Ph.D. | Medium](https://medium.com/@andrew.chamberlain/an-easy-way-to-remember-the-taylor-series-expansion-a7c3f9101063)
2. [Calculus II - Taylor Series (lamar.edu)](https://tutorial.math.lamar.edu/classes/calcii/taylorseries.aspx)
3. 
 
## Dual Numbers and Automatic Differentation
The technique of evaluating a function and its derivative in parallel is called "forward-mode [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)". Our goal is to build up derivatives of complex functions out of the derivatives of small pieces. A [dual number](https://en.wikipedia.org/wiki/Dual_number) is a relatively simple piece of machinery that will help us accomplish this goal.

A [dual number](https://en.wikipedia.org/wiki/Dual_number) is a pair of numbers of the form:

$$
a+b\epsilon
$$
where $a$ and $b$ are real numbers, and $\epsilon$ is an abstract thing, with the property that
$$
\epsilon^2=0
$$

NOTE: This might remind you of the definition of a *complex number* of the form $a+bi$, where $i$ is also a new thing with the property that $i^2=-1$. You are very wise! The bigger idea lurking here is the ["generalized complex number"](https://people.rit.edu/harkin/research/articles/generalized_complex_numbers.pdf), and of course mathematicians have pushed this very far. You might explore the ["Split-complex numbers"](https://en.wikipedia.org/wiki/Split-complex_number) too, which arise when you set $i^2=1$.


If you pass $a+b\epsilon$ in to a function $f$, the result is a dual number
$$
f(a)+Df(a)b\epsilon
$$

you get **both the function and its derivative evaluated at the same time**!

### Taylor Series and DN 
![[Pasted image 20220217221643.png]]
![[Pasted image 20220217221757.png]]

![[Pasted image 20220217221815.png]]
![[Pasted image 20220217221921.png]]

![[Pasted image 20220217221836.png]]


### DN Expansion
To do so, **just expand using [[5.Theories#Taylor Series]] (first two terms) and substitute accordingly**:
![[IMG_20220217_224336.jpg]]

#### Division of DN
![[Pasted image 20220217231026.png]]

### Examples
![[Pasted image 20220217222554.png]]
![[Pasted image 20220217222637.png]]

### References
1. [Dual Numbers and Automatic Differentiation (samritchie.io)](https://samritchie.io/dual-numbers-and-automatic-differentiation/)
2. [Dual Numbers & Automatic Differentiation « The blog at the bottom of the sea (demofox.org)](https://blog.demofox.org/2014/12/30/dual-numbers-automatic-differentiation/)
3. [Dual number - Wikipedia](https://en.wikipedia.org/wiki/Dual_number)


## Optimization
### Concave vs Convex
![[Pasted image 20220226194023.png]]
### Finding local or global min or max using second derivative
![[Pasted image 20220226195951.png]]
Also,
![[Pasted image 20220226200200.png]]


### References
1. [Microsoft Word - La dérivée secondeANG.docx (hec.ca)](https://www.hec.ca/en/cams/help/topics/The_second_derivative.pdf)
2. 