## Logistic regression and conditional probabilities

![[Pasted image 20220207210126.png]]

![[Pasted image 20220207210146.png]]

Goal:
![[Pasted image 20220207210229.png]]

![[Pasted image 20220207212423.png]]


![[Pasted image 20220207210253.png]]
Because sigmoid maps real numbers to \[0, 1\] range, then we can say that this can be interpreted as probability of the data entry belonging to class $Y=1$.

![[Pasted image 20220207210733.png]]

Schematic: 
![[Pasted image 20220207210444.png]]

example: 
![[Pasted image 20220207212534.png]]

Also, see [[2.Answers#SOL-22 CH SOL- 2 19]] for proof.

## Learning the weights of logistic cost function

We wish to find the values of $W$ which give the "best fit" to the data. In the case of linear regression, the sum of the squared deviations of the fit from the data points $(y_i)$ is taken as a measure of the goodness of fit, and the best fit is obtained when that function is minimized. In the case of logistic regression, the measure of goodness of fit is given by the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function "Likelihood function"), which is the probability that the given data set is produced by a particular logistic function:
![[Pasted image 20220207214110.png]]
Convert binary state of $y_i$ to continuous:
![[Pasted image 20220207214153.png]]

![[Pasted image 20220207214206.png]]

Maximize log-likelihood or Minimize negative log-likelihood:
![[Pasted image 20220207215320.png]]

![[Pasted image 20220207215453.png]]


## Some notes
### Chi-square
![[Pasted image 20220210193138.png]]


### Reference
1. [[Raschka - Machine learning with python TF 3rd.pdf#page=89]]
2. [Logistic regression - Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)
3. [Derivative_Rules_Sheet.dvi (ucdavis.edu)](https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf)
4. 
