## Construction of Probability Space

### Probability and Random Variable
![[Pasted image 20220227170745.png]]

![[Pasted image 20220227170805.png]]

![[Pasted image 20220227170824.png]]

## Discrete and Continuous Probabilities
![[Pasted image 20220227170954.png]]
The reason that we use CDF is that PDF for a continuous function for a particular point is almost zero since we can have infinite number of states around the chosen value. That's why it does not make sense and we tend ot use an interval.

### Discrete Probabilities
![[Pasted image 20220227171653.png]]
![[Pasted image 20220227171822.png]]

An example:
![[Pasted image 20220227171711.png]]
![[Pasted image 20220227172758.png]]
![[Pasted image 20220227172831.png]] ^78257d

### Continuous Probabilities
![[Pasted image 20220301130159.png]]

![[Pasted image 20220301135825.png]]
![[Pasted image 20220301140050.png]]
![[Pasted image 20220301140542.png]]
### Contrasting Continuous and Discrete Distributions
![[Pasted image 20220301140702.png]]
![[Pasted image 20220301141023.png]]

![[Pasted image 20220301141235.png]]

![[Pasted image 20220301141719.png]]

## Sum Rule, Product Rule, and Bayes’ Theorem
### Sum Rule
![[Pasted image 20220301170045.png]]

### Product Rule
![[Pasted image 20220301170415.png]]
If we use [[3.Theories#^78257d|Table of an example for probs]], we can say:
$$
\begin{align*}
p(y|x) &=\frac{n_{ij}}{c_{i}},\\
p(x) &= \frac{c_{i}}{N}\\
\\
\implies p(x,y) &= \frac{n_{ij}}{c_{i}} \times \frac{c_{i}}{N} = \frac{n_{ij}}{N} = P(X=x \cap Y=y)
\end{align*}
$$

![[Pasted image 20220301170433.png]]

### Bayes Rule
![[Pasted image 20220301171619.png]]
![[Pasted image 20220301171648.png]]
![[Pasted image 20220301172329.png]]

![[Pasted image 20220301172247.png]]

## Summary Statistic and Independence
### Means and Covariances
![[Pasted image 20220301173604.png]]

![[Pasted image 20220301175415.png]]
#### Expectation
![[Pasted image 20220301173625.png]]

#### Mean
![[Pasted image 20220301174123.png]]
![[Pasted image 20220301174138.png]] ^0473d0

![[Pasted image 20220301174221.png]]

#### Median and Mode
![[Pasted image 20220301174536.png]]

Note: 
![[Pasted image 20220301175621.png]]

#### Covariance
![[Pasted image 20220301181245.png]] ^15778a

![[Pasted image 20220301181310.png]]

Note: Why does Positive Semidefinite matter?
![[Pasted image 20220307200043.png]]
Ref: https://stats.stackexchange.com/a/224155/216826

![[Pasted image 20220301181735.png]]
![[Pasted image 20220301181431.png]]

#### Correlation
![[Pasted image 20220301181724.png]]

#### Empirical Mean and Covariance (vs Population Mean and Convariance)
![[Pasted image 20220301182415.png]]

#### Three Expressions for the Variance
Standard defintion of Variance is from the definiton of [[3.Theories#^15778a|Covariance]]:
![[Pasted image 20220307201408.png]]
[[3.Theories#^0473d0|Equation 6.32]] is used to compute *mean*.

To calculate Equation 6.43, we need a two-pass algorithm: 
1. calculate $\mu$ 
2. calulate variance using obtained $\mu$ from step 1.

To skip this:
![[Pasted image 20220307202037.png]]
Note that this method is numerically unstable (If the two terms are huge and approximately equal, we may suffer from an unnecessary loss of numerical precision in floating-point arithmetic.) but useful for machine learning in [[Bias-Variance decomposition]].

![[Pasted image 20220307202509.png]]

We see that (6.45) is twice the raw-score expression (6.44):
![[Pasted image 20220307202711.png]]

### Sums and Transformations of Random Variables
![[Pasted image 20220307204123.png]]

![[Pasted image 20220307204103.png]]

### Statistical Independence
![[Pasted image 20220307204721.png]]
![[Pasted image 20220307205023.png]]

![[Pasted image 20220307205447.png]]

### Inner Products of Random Variables
![[Pasted image 20220307211606.png]]
![[Pasted image 20220307210501.png]]

#### Distance In Comparing Probability Distributions
![[Pasted image 20220307211803.png]]
![[Pasted image 20220307211837.png]]

## Gaussian Distribution
 We will use it to define the likelihood and prior for linear regression, and consider a mixture of Gaussians for density estimation.

### Definition
![[Pasted image 20220308172846.png]]

### Marginals and Conditionals of Gaussians are Gaussians
![[Pasted image 20220308175046.png]]
![[Pasted image 20220308175221.png]]

![[Pasted image 20220308175757.png]]

An example:
![[Pasted image 20220308180803.png]]
![[Pasted image 20220308180820.png]]

### Product of Gaussian Densities
![[Pasted image 20220308181618.png]]

### Sums and Linear Transformations
![[Pasted image 20220308184207.png]]
Equation 4.46-4.49 can be found in [[3.Theories#Sums and Transformations of Random Variables|Sum and Transformation rules]].

#### Mixture of Gaussian `densities` vs Mixture of Gaussian `random variables`
![[Pasted image 20220308184332.png]]

![[Pasted image 20220308184356.png]]

#### Proof
![[Pasted image 20220308191705.png]]

![[Pasted image 20220308221443.png]]
![[Pasted image 20220308221521.png]]

#### Affine Transformation of Gaussian is Gaussian
![[Pasted image 20220308222952.png]]

### Sampling from Multivariate Gaussian Distributions
![[Pasted image 20220308224730.png]]

Box-Muller transform:
![[Pasted image 20220308224811.png]]
Ref: [Box–Muller transform - Wikipedia](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)

##  Conjugacy and the Exponential Family


## Other Notes
### PMF vs PDF
 $f(x)$, that defines the probability of the discrete random variable X taking on a particular value x. When we take all the possible values (sample space) and associated probabilities into consideration, it is called a discrete probability distribution (as defined by a pmf).

 **The concept of probability for a given value when the value is on a continous scale doesn’t make sense**. Instead, what we do is “discretize” the sample space so that we can work in intervals instead of individual values. When we talk about the proportion of outcomes falling into an interval like this, then this is called a “probability mass”. As the probability mass is dependent on the interval size, the “probability density” is used to represent the ratio of the probability mass to interval size.

 
### References
1. [Probability Distributions and their Mass/Density Functions (tinyheero.github.io)](https://tinyheero.github.io/2016/03/17/prob-distr.html)
2. 